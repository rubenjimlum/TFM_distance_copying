{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1056a3-b8d4-441b-8d88-5639f180ce93",
   "metadata": {},
   "source": [
    "# Experiment 1: Comparison of copying algorithms in dimension 2 with SNN copies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291e994-2d10-48c9-b181-37d7b8f947c8",
   "metadata": {},
   "source": [
    "In this notebook, we perform computations corresponding to Experiment 1, described in the thesis report. Specifically, we use Algorithms 1 and 2 to train Small Neural Network copies (SNN) in the two-dimensional datasets, for each of the 3 different black box models considered in the experiment, that later we compare to the corresponding SNN hard copies. Computations are limited to 1,000,000 synthetic samples and 240 seconds. Results are stored in the corresponding results folder present in the repository.\n",
    "\n",
    "As a remark, this particular notebook corresponds to the execution with seed 45. Nevertheless, the computations have been repeated for five different seeds (41, 42, 43, 44, and 45), aiming to increase the reliability and significance of the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b880cc1a-acb3-484d-b035-423955c83c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports\n",
    "import numpy as np\n",
    "import os\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from time import perf_counter\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "import pickle\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras import Model as KerasModel\n",
    "import gc\n",
    "\n",
    "\n",
    "original_cwd = os.getcwd()\n",
    "os.chdir('../utils')\n",
    "from utils import *\n",
    "os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451e0d71-6e98-4375-8206-71511370cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "seed = 45\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Create a wrapper for our Neural network black boxes\n",
    "def bbmodelW(x):\n",
    "    if isinstance(bbmodel, tf.keras.models.Model):\n",
    "        return np.where(bbmodel(x) > 0.5, 1, -1).flatten()\n",
    "    return np.where(bbmodel.predict(x) > 0.5, 1, -1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5b343-1f56-4115-b0a7-a3390d6a089a",
   "metadata": {},
   "source": [
    "## Overlapping Gaussians dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836c1cee-e453-402e-bb39-b57148a2c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 800\n",
      "Size of test dataset: 200\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "data_train = np.load(\"../data/Synth_dataset_1_train.npz\")\n",
    "X_train = data_train[\"X\"]\n",
    "y_train = data_train[\"y\"]\n",
    "\n",
    "data_test = np.load(\"../data/Synth_dataset_1_test.npz\")\n",
    "X_test = data_test[\"X\"]\n",
    "y_test = data_test[\"y\"]\n",
    "\n",
    "print(\"Size of training dataset:\", len(X_train))\n",
    "print(\"Size of test dataset:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d42fe5-0952-41f4-9caf-60127dbab7ef",
   "metadata": {},
   "source": [
    "### Black box 1: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a957e1ef-3c72-4179-8882-b3fe1cd455eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = RandomForestClassifier(max_depth=10, min_samples_leaf=5)\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 2))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74590347-b4e1-4232-bc44-44ab17c43c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 1659 points in 240.11 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(2, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64985c0-d7de-4652-86f1-61fc001760b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 1660 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_SNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b30038d-45be-490c-abca-eeca818d24fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(2, -1, 1, 40_000, 25, 1_200, 1, 0.1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9fb1a6-eea6-4337-9c56-f011cad93851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_SNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcde2cc4-2cc7-4780-a5af-83508d01d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=2, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3319b495-662e-4df6-b5af-835f575a6300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_SNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b81be02-0268-44f5-a655-f344f4f64a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17061"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS1_1_1_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277015e-c423-4185-a76c-74b0febae547",
   "metadata": {},
   "source": [
    "### Black box 2: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec94b8c1-5744-4c6a-9574-30db19d304d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = HistGradientBoostingClassifier()\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 2))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a89587-5d9e-4f43-810f-0ef3cba1fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 1907 points in 240.08 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(2, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d855f8d-6467-4456-9032-382bd3109bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 1908 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_SNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a673228-80e3-4b51-99ff-e747f16827cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(2, -1, 1, 40_000, 25, 1_200, 1, 0.1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f63f347-e6bd-4b39-9401-05f7c1174154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_SNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f885536f-c91d-4724-82ae-7e095737d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=2, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56b7045-c0f0-4253-a16d-c2ee579ba95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_SNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04cc2ef0-0650-4cda-9751-ff4f8fbb5fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36118"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS1_2_1_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b09ec1-2760-4ad9-aced-fd546d0aae2b",
   "metadata": {},
   "source": [
    "### Black box 3: Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "745be692-083e-49f9-bad3-28461f7c1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation = \"relu\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(32, activation = \"relu\"),\n",
    "        layers.Dense(16, activation = \"relu\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "bbmodel.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy())\n",
    "bbmodel.fit(X_train, y_train, batch_size=32, epochs=50, verbose = 0)\n",
    "\n",
    "yhat = bbmodelW(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == (2*y_test-1))\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 2))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ea1ac74-05fe-4bc4-b5b1-257a8660335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 4283 points in 240.03 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(2, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8479e207-2e7e-4e91-99ef-ee3c89dca16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 4284 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_SNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e49f774c-8d17-4b64-954f-999e11efea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(2, -1, 1, 40_000, 25, 1_200, 1, 0.1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b374957-687e-4413-a3b1-774713cbda96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_SNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e3e17e6-bf7c-4fe5-9556-8cde63a3bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=2, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5eed29e4-07e5-4efa-8eb2-07581c05813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_SNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fb2d515-f22b-49ff-a241-786aa28b1768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28629"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS1_3_1_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86d5c9-54c8-44f5-8a13-a463c765f8b4",
   "metadata": {},
   "source": [
    "## Two spirals dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e13c1ed-d322-4b1a-81bb-0af10d7bfb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 8000\n",
      "Size of test dataset: 2000\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "data_train = np.load(\"../data/Synth_dataset_2_train.npz\")\n",
    "X_train = data_train[\"X\"]\n",
    "y_train = data_train[\"y\"]\n",
    "\n",
    "data_test = np.load(\"../data/Synth_dataset_2_test.npz\")\n",
    "X_test = data_test[\"X\"]\n",
    "y_test = data_test[\"y\"]\n",
    "\n",
    "print(\"Size of training dataset:\", len(X_train))\n",
    "print(\"Size of test dataset:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc4566-9069-4e2e-bab9-ffd4f8ff35e8",
   "metadata": {},
   "source": [
    "### Black box 1: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2da2ff22-f33e-49cd-a2f8-aa5de7bf1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = RandomForestClassifier(max_depth=10, min_samples_leaf=5)\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 2))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b983d447-55b3-422e-a845-5c32045e6651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 1425 points in 240.16 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(2, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4d4703f-da67-43e0-948e-9ca7d44f7553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 1426 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_SNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2759f35-c04c-430e-bdd4-62280505fae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(2, -1, 1, 40_000, 25, 1_200, 1, 0.1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa5fc0f6-be4e-434a-9bc9-f3c79b393f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_SNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ec2a89a-4365-48ff-88fb-1d3a20040f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=2, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce968d8a-a76d-41f0-869e-1327bddcdd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_SNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57d7a999-8c98-439c-aeed-e09b14b8f488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12559"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS2_1_1_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd829d-8179-4499-a10e-90ae00d26daa",
   "metadata": {},
   "source": [
    "### Black box 2: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a7a5c03-766d-4a7e-92f5-43ad1284f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = HistGradientBoostingClassifier()\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 2))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aab40f07-47fe-4c2c-b558-99e2890d7a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 1861 points in 240.05 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(2, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f6b1305-050a-4df7-8ebb-54a4d3a421e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 1862 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_SNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b713807-995c-4039-80be-43a58d7e7cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(2, -1, 1, 40_000, 25, 1_200, 1, 0.1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b2e5bda-0cc4-4dda-90cc-1d2d7eb0f85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_SNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a2e2874-dc87-4a1a-a26f-baa4f3a6c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=2, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b082049-8f74-4bae-9669-385b4814c820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_SNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e51bfc23-ff74-4d16-bb27-6ccf973ea4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58180"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS2_2_1_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59069f-461d-43f6-a53a-9a04d7e04183",
   "metadata": {},
   "source": [
    "### Black box 3: Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5cabde6-88b9-49da-b676-fb5c831a19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation = \"relu\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(32, activation = \"relu\"),\n",
    "        layers.Dense(16, activation = \"relu\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "bbmodel.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy())\n",
    "bbmodel.fit(X_train, y_train, batch_size=32, epochs=50, verbose = 0)\n",
    "\n",
    "yhat = bbmodelW(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == (2*y_test-1))\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 2))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c59c3be6-19da-45ad-89c3-daf97d4a091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 4125 points in 240.02 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(2, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d539c913-ee15-4402-8c84-a58f289504d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 4126 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_SNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92443e74-8c5c-495c-a34f-9768cf2999f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(2, -1, 1, 40_000, 25, 1_200, 1, 0.1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "312433bf-d290-4d46-a242-059e6ac7662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_SNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b869f7c-fdc5-4259-98df-cb3d9c051e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=2, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4e2cd6f-40b7-40fc-beef-b03c1be3066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_SNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1d37bb6-9675-4bb6-bcc8-5efd45bf2cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60674"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS2_3_1_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a699d43-52e6-4bc9-aa88-b45b08aa93d0",
   "metadata": {},
   "source": [
    "## Space-filling and convoluted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cf8db7c-00a1-4c47-a298-40cefe70402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 8000\n",
      "Size of test dataset: 2000\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "data_train = np.load(\"../data/Synth_dataset_3_train.npz\")\n",
    "X_train = data_train[\"X\"]\n",
    "y_train = data_train[\"y\"]\n",
    "\n",
    "data_test = np.load(\"../data/Synth_dataset_3_test.npz\")\n",
    "X_test = data_test[\"X\"]\n",
    "y_test = data_test[\"y\"]\n",
    "\n",
    "print(\"Size of training dataset:\", len(X_train))\n",
    "print(\"Size of test dataset:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e08e21-346a-47b0-9c5b-a037b0dc16c0",
   "metadata": {},
   "source": [
    "### Black box 1: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7c40b85-029d-4af8-8b1f-05ba9fdea0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = RandomForestClassifier(max_depth=10, min_samples_leaf=5)\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 2))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4de92dc3-8bdd-4a77-886a-152a01862c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 993 points in 240.16 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(2, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d308829-a8a8-42d0-91ef-3a3163e9dcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 994 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_SNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51a50cbe-c48c-4cb5-bc98-850a4a5e426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 700025 points in 255.08 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(2, -1, 1, 40_000, 25, 1_200, 1, 0.1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c4793ee-5782-447c-b24c-f519a84bfcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 700025 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_SNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "907a5b1b-8df7-4235-86ac-69d17032fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=2, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6ebe940-8161-4e52-a32a-5e05cba53b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_SNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da91f446-3332-4b82-bf14-5167c13c715f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57508"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS3_1_1_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dc1c6-880b-4198-9d6c-2e1cd3aa76b1",
   "metadata": {},
   "source": [
    "### Black box 2: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6909dcbd-a30a-46b4-aa39-cfdd8c18da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = HistGradientBoostingClassifier()\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 2))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a89a1730-8dc9-4289-90c9-7ac186c3ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 2077 points in 240.1 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(2, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "19b25ad0-833f-413d-8bd9-49fa8f2820f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 2078 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_SNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8f92488-4d88-4d8e-b83f-3e2f67c07af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(2, -1, 1, 40_000, 25, 1_200, 1, 0.1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4dfc465-2ce9-4b58-8a84-f8d99a2eca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_SNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19360180-1280-4605-84e8-14248a38e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=2, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2a3d20f-6efb-4f1f-9eba-660a8588b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_SNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a9a59b0-f466-459d-a9e6-83bea6ac2798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58180"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS3_2_1_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ea2c6-8c54-4606-815e-0ad1337013bf",
   "metadata": {},
   "source": [
    "### Black box 3: Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e1e7165-742d-43d0-8f56-dbdd73169d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation = \"relu\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(32, activation = \"relu\"),\n",
    "        layers.Dense(16, activation = \"relu\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "bbmodel.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy())\n",
    "bbmodel.fit(X_train, y_train, batch_size=32, epochs=50, verbose=0)\n",
    "\n",
    "yhat = bbmodelW(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == (2*y_test-1))\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 2))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1912bdda-55b2-435d-bc91-bb2836a63ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 3639 points in 240.01 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(2, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe04a224-265c-40af-85c5-1b65b3a31f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 3640 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_SNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "530fda38-e1e3-477a-83ca-5a9c8a1cf6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(2, -1, 1, 40_000, 25, 1_200, 1, 0.1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be462cb5-c949-40b7-8df7-8542c53fe0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_SNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "109f088f-083e-436b-8eb9-bfe969df4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=2, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32ba977b-922c-443a-8c20-b6a96836649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_SNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b9822ad-130b-466a-8c9c-7d0d01e95f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60612"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS3_3_1_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
