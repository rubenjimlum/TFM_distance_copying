{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1056a3-b8d4-441b-8d88-5639f180ce93",
   "metadata": {},
   "source": [
    "# Experiment 2: Comparison across values of $\\alpha$ in high dimensions with LNN copies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290f420-a8e4-43c2-b3d0-6a2b484dc8ab",
   "metadata": {},
   "source": [
    "In this notebook, we perform computations corresponding to Experiment 2, described in the thesis report. Specifically, we train Large Neural Networks copies (LNN) for values of $\\alpha \\in \\{0, 0.25, 0.5, 0.75, 1, 1.25\\}$ in the UCI high-dimensional datasets, for each of the 3 different black box models considered in the experiment, that later we compare to the corresponding LNN hard copies. All copies are trained on the same 1,000,000 synthetic samples generated with Algorithm 2. Results are stored in the corresponding results folder present in the repository.\n",
    "\n",
    "As a remark, this particular notebook corresponds to the execution with seed 45. Nevertheless, the computations have been repeated for five different seeds (41, 42, 43, 44, and 45), aiming to increase the reliability and significance of the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b880cc1a-acb3-484d-b035-423955c83c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports\n",
    "import numpy as np\n",
    "import os\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from time import perf_counter\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "import pickle\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras import Model as KerasModel\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from scipy.stats import qmc\n",
    "\n",
    "\n",
    "original_cwd = os.getcwd()\n",
    "os.chdir('../utils')\n",
    "from utils import *\n",
    "os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451e0d71-6e98-4375-8206-71511370cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "seed = 45\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Create a wrapper for our Neural network black boxes\n",
    "def bbmodelW(x):\n",
    "    if isinstance(bbmodel, tf.keras.models.Model):\n",
    "        return np.where(bbmodel(x) > 0.5, 1, -1).flatten()\n",
    "    return np.where(bbmodel.predict(x) > 0.5, 1, -1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5b343-1f56-4115-b0a7-a3390d6a089a",
   "metadata": {},
   "source": [
    "## Breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836c1cee-e453-402e-bb39-b57148a2c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features.values\n",
    "X = normalize(X)\n",
    "y = np.where(breast_cancer_wisconsin_diagnostic.data.targets.values.flatten() == 'M', 1.,0.)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d42fe5-0952-41f4-9caf-60127dbab7ef",
   "metadata": {},
   "source": [
    "### Black box 1: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a957e1ef-3c72-4179-8882-b3fe1cd455eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = RandomForestClassifier(max_depth=10, min_samples_leaf=5)\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 30))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6e120a-3524-4833-a959-b4d9e6e7fd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts, data, lab = generate_distances_algo3(30, -1, 1, 40_000, 25, 2_000, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa6102e-b266-464b-8ed5-d73f8ca81913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations done for the hard copy\n",
      "Computations done for the α = 0 copy\n",
      "Computations done for the α = 0.25 copy\n",
      "Computations done for the α = 0.5 copy\n",
      "Computations done for the α = 0.75 copy\n",
      "Computations done for the α = 1 copy\n",
      "Computations done for the α = 1.25 copy\n"
     ]
    }
   ],
   "source": [
    "acc, efe_unif = train_copy_LNN_conti_hdim(data, lab, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e4acdf-cee3-443e-935d-1564d3bbe9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"../results/alpha_DS4_1_3_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump({\"acc\": acc, \"efe_unif\": efe_unif}, f)\n",
    "\n",
    "del pts\n",
    "del efe_unif\n",
    "del acc\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277015e-c423-4185-a76c-74b0febae547",
   "metadata": {},
   "source": [
    "### Black box 2: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec94b8c1-5744-4c6a-9574-30db19d304d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = HistGradientBoostingClassifier()\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 30))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32560ffc-2fa1-42ba-bf4e-f0fcc27bfcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts, data, lab = generate_distances_algo3(30, -1, 1, 40_000, 25, 2_000, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f863e3f-5986-408b-9708-c65517f2c167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations done for the hard copy\n",
      "Computations done for the α = 0 copy\n",
      "Computations done for the α = 0.25 copy\n",
      "Computations done for the α = 0.5 copy\n",
      "Computations done for the α = 0.75 copy\n",
      "Computations done for the α = 1 copy\n",
      "Computations done for the α = 1.25 copy\n"
     ]
    }
   ],
   "source": [
    "acc, efe_unif = train_copy_LNN_conti_hdim(data, lab, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f8d7ea-3167-48f7-ac24-ae960f3df4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"../results/alpha_DS4_2_3_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump({\"acc\": acc, \"efe_unif\": efe_unif}, f)\n",
    "\n",
    "del pts\n",
    "del efe_unif\n",
    "del acc\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b09ec1-2760-4ad9-aced-fd546d0aae2b",
   "metadata": {},
   "source": [
    "### Black box 3: Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "745be692-083e-49f9-bad3-28461f7c1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation = \"relu\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(32, activation = \"relu\"),\n",
    "        layers.Dense(16, activation = \"relu\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "bbmodel.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy())\n",
    "bbmodel.fit(X_train, y_train, batch_size=32, epochs=50, verbose = 0)\n",
    "\n",
    "yhat = bbmodelW(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == (2*y_test-1))\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 30))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d76b3537-72f3-4bc2-acbd-8345889adf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts, data, lab = generate_distances_algo3(30, -1, 1, 40_000, 25, 2_000, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb296830-2b20-4047-bc03-d4330ede7933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations done for the hard copy\n",
      "Computations done for the α = 0 copy\n",
      "Computations done for the α = 0.25 copy\n",
      "Computations done for the α = 0.5 copy\n",
      "Computations done for the α = 0.75 copy\n",
      "Computations done for the α = 1 copy\n",
      "Computations done for the α = 1.25 copy\n"
     ]
    }
   ],
   "source": [
    "acc, efe_unif = train_copy_LNN_conti_hdim(data, lab, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe15717-85cc-42a0-a851-f1ca5999c870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2027"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"../results/alpha_DS4_3_3_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump({\"acc\": acc, \"efe_unif\": efe_unif}, f)\n",
    "\n",
    "del pts\n",
    "del efe_unif\n",
    "del acc\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86d5c9-54c8-44f5-8a13-a463c765f8b4",
   "metadata": {},
   "source": [
    "## Rice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e13c1ed-d322-4b1a-81bb-0af10d7bfb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "rice_cammeo_and_osmancik = fetch_ucirepo(id=545) \n",
    "  \n",
    "X = rice_cammeo_and_osmancik.data.features.values\n",
    "X = normalize(X)\n",
    "y = np.where(rice_cammeo_and_osmancik.data.targets.values.flatten() == 'Cammeo', 1.,0.)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc4566-9069-4e2e-bab9-ffd4f8ff35e8",
   "metadata": {},
   "source": [
    "### Black box 1: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2da2ff22-f33e-49cd-a2f8-aa5de7bf1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = RandomForestClassifier(max_depth=10, min_samples_leaf=5)\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 7))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34514599-077a-400c-a742-07c1e83a07a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts, data, lab = generate_distances_algo3(7, -1, 1, 40_000, 25, 2_000, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68594837-1fef-40c9-a11a-07d935ef9836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations done for the hard copy\n",
      "Computations done for the α = 0 copy\n",
      "Computations done for the α = 0.25 copy\n",
      "Computations done for the α = 0.5 copy\n",
      "Computations done for the α = 0.75 copy\n",
      "Computations done for the α = 1 copy\n",
      "Computations done for the α = 1.25 copy\n"
     ]
    }
   ],
   "source": [
    "acc, efe_unif = train_copy_LNN_conti_hdim(data, lab, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "830b0c06-ff89-4d5b-aadd-27229ab4ba84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"../results/alpha_DS5_1_3_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump({\"acc\": acc, \"efe_unif\": efe_unif}, f)\n",
    "\n",
    "del pts\n",
    "del efe_unif\n",
    "del acc\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd829d-8179-4499-a10e-90ae00d26daa",
   "metadata": {},
   "source": [
    "### Black box 2: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a7a5c03-766d-4a7e-92f5-43ad1284f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = HistGradientBoostingClassifier()\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 7))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bd74d9f-a9a8-4a09-9497-307e7958f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts, data, lab = generate_distances_algo3(7, -1, 1, 40_000, 25, 2_000, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d69b0f3a-ed73-40b7-8ab5-e17c6e5aa778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations done for the hard copy\n",
      "Computations done for the α = 0 copy\n",
      "Computations done for the α = 0.25 copy\n",
      "Computations done for the α = 0.5 copy\n",
      "Computations done for the α = 0.75 copy\n",
      "Computations done for the α = 1 copy\n",
      "Computations done for the α = 1.25 copy\n"
     ]
    }
   ],
   "source": [
    "acc, efe_unif = train_copy_LNN_conti_hdim(data, lab, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f9a2936-36c0-4ce4-9638-2ee9e0285236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"../results/alpha_DS5_2_3_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump({\"acc\": acc, \"efe_unif\": efe_unif}, f)\n",
    "\n",
    "del pts\n",
    "del efe_unif\n",
    "del acc\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59069f-461d-43f6-a53a-9a04d7e04183",
   "metadata": {},
   "source": [
    "### Black box 3: Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5cabde6-88b9-49da-b676-fb5c831a19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation = \"relu\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(32, activation = \"relu\"),\n",
    "        layers.Dense(16, activation = \"relu\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "bbmodel.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy())\n",
    "bbmodel.fit(X_train, y_train, batch_size=32, epochs=50, verbose = 0)\n",
    "\n",
    "yhat = bbmodelW(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == (2*y_test-1))\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 7))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dc22529-2c35-4bf4-b083-67e633881270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts, data, lab = generate_distances_algo3(7, -1, 1, 40_000, 25, 2_000, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb2a7b83-1b3b-49b5-a3fb-85150bd1a11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations done for the hard copy\n",
      "Computations done for the α = 0 copy\n",
      "Computations done for the α = 0.25 copy\n",
      "Computations done for the α = 0.5 copy\n",
      "Computations done for the α = 0.75 copy\n",
      "Computations done for the α = 1 copy\n",
      "Computations done for the α = 1.25 copy\n"
     ]
    }
   ],
   "source": [
    "acc, efe_unif = train_copy_LNN_conti_hdim(data, lab, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2620f039-8bfd-4c9f-b20b-d9bc6e780492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2027"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"../results/alpha_DS5_3_3_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump({\"acc\": acc, \"efe_unif\": efe_unif}, f)\n",
    "\n",
    "del pts\n",
    "del efe_unif\n",
    "del acc\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a699d43-52e6-4bc9-aa88-b45b08aa93d0",
   "metadata": {},
   "source": [
    "## Connectionist bench (mines vs rocks) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cf8db7c-00a1-4c47-a298-40cefe70402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "connectionist_bench_sonar_mines_vs_rocks = fetch_ucirepo(id=151) \n",
    "\n",
    "X = connectionist_bench_sonar_mines_vs_rocks.data.features.values \n",
    "X = normalize(X) \n",
    "y = np.where(connectionist_bench_sonar_mines_vs_rocks.data.targets.values.flatten() == 'M', 1.,0.)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e08e21-346a-47b0-9c5b-a037b0dc16c0",
   "metadata": {},
   "source": [
    "### Black box 1: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7c40b85-029d-4af8-8b1f-05ba9fdea0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = RandomForestClassifier(max_depth=10, min_samples_leaf=5)\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 60))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "498af880-0000-43c4-845b-bf277d7f4d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts, data, lab = generate_distances_algo3(60, -1, 1, 40_000, 25, 2_000, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "383b2171-5734-4d37-ac75-bd394843712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations done for the hard copy\n",
      "Computations done for the α = 0 copy\n",
      "Computations done for the α = 0.25 copy\n",
      "Computations done for the α = 0.5 copy\n",
      "Computations done for the α = 0.75 copy\n",
      "Computations done for the α = 1 copy\n",
      "Computations done for the α = 1.25 copy\n"
     ]
    }
   ],
   "source": [
    "acc, efe_unif = train_copy_LNN_conti_hdim(data, lab, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a3726f9-d299-499e-a2ce-374c1ce74c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"../results/alpha_DS6_1_3_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump({\"acc\": acc, \"efe_unif\": efe_unif}, f)\n",
    "\n",
    "del pts\n",
    "del efe_unif\n",
    "del acc\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dc1c6-880b-4198-9d6c-2e1cd3aa76b1",
   "metadata": {},
   "source": [
    "### Black box 2: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6909dcbd-a30a-46b4-aa39-cfdd8c18da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = HistGradientBoostingClassifier()\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 60))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86c94e7c-f177-4f3a-9c5a-8c82c760ce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts, data, lab = generate_distances_algo3(60, -1, 1, 40_000, 25, 2_000, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d7aa3d2-44b2-4412-9e45-4f74f0c9e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations done for the hard copy\n",
      "Computations done for the α = 0 copy\n",
      "Computations done for the α = 0.25 copy\n",
      "Computations done for the α = 0.5 copy\n",
      "Computations done for the α = 0.75 copy\n",
      "Computations done for the α = 1 copy\n",
      "Computations done for the α = 1.25 copy\n"
     ]
    }
   ],
   "source": [
    "acc, efe_unif = train_copy_LNN_conti_hdim(data, lab, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d72fcb9-7c8d-4645-9834-072dcf4947d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"../results/alpha_DS6_2_3_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump({\"acc\": acc, \"efe_unif\": efe_unif}, f)\n",
    "\n",
    "del pts\n",
    "del efe_unif\n",
    "del acc\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ea2c6-8c54-4606-815e-0ad1337013bf",
   "metadata": {},
   "source": [
    "### Black box 3: Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e1e7165-742d-43d0-8f56-dbdd73169d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation = \"relu\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(32, activation = \"relu\"),\n",
    "        layers.Dense(16, activation = \"relu\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "bbmodel.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy())\n",
    "bbmodel.fit(X_train, y_train, batch_size=32, epochs=50, verbose=0)\n",
    "\n",
    "yhat = bbmodelW(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == (2*y_test-1))\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 60))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72f972ee-adf3-41e9-a9c0-08cd53477768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts, data, lab = generate_distances_algo3(60, -1, 1, 40_000, 25, 2_000, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d56ed6e-250e-4dd8-8feb-81c9aa326408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations done for the hard copy\n",
      "Computations done for the α = 0 copy\n",
      "Computations done for the α = 0.25 copy\n",
      "Computations done for the α = 0.5 copy\n",
      "Computations done for the α = 0.75 copy\n",
      "Computations done for the α = 1 copy\n",
      "Computations done for the α = 1.25 copy\n"
     ]
    }
   ],
   "source": [
    "acc, efe_unif = train_copy_LNN_conti_hdim(data, lab, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab56e3c8-724c-4bd1-a22c-6d788c78f437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2027"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f\"../results/alpha_DS6_3_3_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump({\"acc\": acc, \"efe_unif\": efe_unif}, f)\n",
    "\n",
    "del pts\n",
    "del efe_unif\n",
    "del acc\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
