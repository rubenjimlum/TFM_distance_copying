{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1056a3-b8d4-441b-8d88-5639f180ce93",
   "metadata": {},
   "source": [
    "# Experiment 1: Comparison of copying algorithms in higher dimensions with MNN copies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3c7c9-7d06-4d05-9a08-a4b6ce460778",
   "metadata": {},
   "source": [
    "In this notebook, we perform computations corresponding to Experiment 1, described in the thesis report. Specifically, we use Algorithms 1 and 2 to train Medium Neural Network copies (MNN) in the UCI high-dimensional datasets, for each of the 3 different black box models considered in the experiment, that later we compare to the corresponding MNN hard copies. Computations are limited to 1,000,000 synthetic samples and 240 seconds. Results are stored in the corresponding results folder present in the repository.\n",
    "\n",
    "As a remark, this particular notebook corresponds to the execution with seed 45. Nevertheless, the computations have been repeated for five different seeds (41, 42, 43, 44, and 45), aiming to increase the reliability and significance of the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b880cc1a-acb3-484d-b035-423955c83c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports\n",
    "import numpy as np\n",
    "import os\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from time import perf_counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import matplotlib.colors as mcolors\n",
    "import random\n",
    "import pickle\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras import Model as KerasModel\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import gc\n",
    "\n",
    "\n",
    "original_cwd = os.getcwd()\n",
    "os.chdir('../utils')\n",
    "from utils import *\n",
    "os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451e0d71-6e98-4375-8206-71511370cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "seed = 45\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Create a wrapper for our Neural network black boxes\n",
    "def bbmodelW(x):\n",
    "    if isinstance(bbmodel, tf.keras.models.Model):\n",
    "        return np.where(bbmodel(x) > 0.5, 1, -1).flatten()\n",
    "    return np.where(bbmodel.predict(x) > 0.5, 1, -1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5b343-1f56-4115-b0a7-a3390d6a089a",
   "metadata": {},
   "source": [
    "## Breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836c1cee-e453-402e-bb39-b57148a2c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 455\n",
      "Size of test dataset: 114\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features.values\n",
    "X = normalize(X)\n",
    "y = np.where(breast_cancer_wisconsin_diagnostic.data.targets.values.flatten() == 'M', 1.,0.)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "print(\"Size of training dataset:\", len(X_train))\n",
    "print(\"Size of test dataset:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d42fe5-0952-41f4-9caf-60127dbab7ef",
   "metadata": {},
   "source": [
    "### Black box 1: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a957e1ef-3c72-4179-8882-b3fe1cd455eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = RandomForestClassifier(max_depth=10, min_samples_leaf=5)\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 30))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74590347-b4e1-4232-bc44-44ab17c43c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 2525 points in 240.06 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(30, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64985c0-d7de-4652-86f1-61fc001760b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 2526 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_MNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b30038d-45be-490c-abca-eeca818d24fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(30, -1, 1, 40_000, 25, 1_200, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9fb1a6-eea6-4337-9c56-f011cad93851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_MNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcde2cc4-2cc7-4780-a5af-83508d01d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=30, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3319b495-662e-4df6-b5af-835f575a6300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_MNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b81be02-0268-44f5-a655-f344f4f64a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS4_1_2_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277015e-c423-4185-a76c-74b0febae547",
   "metadata": {},
   "source": [
    "### Black box 2: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec94b8c1-5744-4c6a-9574-30db19d304d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = HistGradientBoostingClassifier()\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 30))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a89587-5d9e-4f43-810f-0ef3cba1fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 2367 points in 240.06 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(30, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d855f8d-6467-4456-9032-382bd3109bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 2368 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_MNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a673228-80e3-4b51-99ff-e747f16827cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(30, -1, 1, 40_000, 25, 1_200, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f63f347-e6bd-4b39-9401-05f7c1174154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_MNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f885536f-c91d-4724-82ae-7e095737d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=30, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56b7045-c0f0-4253-a16d-c2ee579ba95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_MNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04cc2ef0-0650-4cda-9751-ff4f8fbb5fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13493"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS4_2_2_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b09ec1-2760-4ad9-aced-fd546d0aae2b",
   "metadata": {},
   "source": [
    "### Black box 3: Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "745be692-083e-49f9-bad3-28461f7c1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation = \"relu\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(32, activation = \"relu\"),\n",
    "        layers.Dense(16, activation = \"relu\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "bbmodel.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy())\n",
    "bbmodel.fit(X_train, y_train, batch_size=32, epochs=50, verbose = 0)\n",
    "\n",
    "yhat = bbmodelW(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == (2*y_test-1))\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 30))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ea1ac74-05fe-4bc4-b5b1-257a8660335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 4161 points in 240.01 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(30, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8479e207-2e7e-4e91-99ef-ee3c89dca16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 4162 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_MNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e49f774c-8d17-4b64-954f-999e11efea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(30, -1, 1, 40_000, 25, 1_200, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b374957-687e-4413-a3b1-774713cbda96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_MNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e3e17e6-bf7c-4fe5-9556-8cde63a3bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=30, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5eed29e4-07e5-4efa-8eb2-07581c05813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_MNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fb2d515-f22b-49ff-a241-786aa28b1768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36540"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS4_3_2_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac86d5c9-54c8-44f5-8a13-a463c765f8b4",
   "metadata": {},
   "source": [
    "## Rice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e13c1ed-d322-4b1a-81bb-0af10d7bfb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 3048\n",
      "Size of test dataset: 762\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "rice_cammeo_and_osmancik = fetch_ucirepo(id=545) \n",
    "  \n",
    "X = rice_cammeo_and_osmancik.data.features.values\n",
    "X = normalize(X)\n",
    "y = np.where(rice_cammeo_and_osmancik.data.targets.values.flatten() == 'Cammeo', 1.,0.)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "print(\"Size of training dataset:\", len(X_train))\n",
    "print(\"Size of test dataset:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc4566-9069-4e2e-bab9-ffd4f8ff35e8",
   "metadata": {},
   "source": [
    "### Black box 1: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2da2ff22-f33e-49cd-a2f8-aa5de7bf1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = RandomForestClassifier(max_depth=10, min_samples_leaf=5)\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 7))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b983d447-55b3-422e-a845-5c32045e6651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 1588 points in 240.01 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(7, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4d4703f-da67-43e0-948e-9ca7d44f7553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 1589 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_MNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2759f35-c04c-430e-bdd4-62280505fae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(7, -1, 1, 40_000, 25, 1_200, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa5fc0f6-be4e-434a-9bc9-f3c79b393f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_MNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ec2a89a-4365-48ff-88fb-1d3a20040f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=7, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce968d8a-a76d-41f0-869e-1327bddcdd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_MNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57d7a999-8c98-439c-aeed-e09b14b8f488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18300"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS5_1_2_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd829d-8179-4499-a10e-90ae00d26daa",
   "metadata": {},
   "source": [
    "### Black box 2: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a7a5c03-766d-4a7e-92f5-43ad1284f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = HistGradientBoostingClassifier()\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 7))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aab40f07-47fe-4c2c-b558-99e2890d7a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 1990 points in 240.04 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(7, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f6b1305-050a-4df7-8ebb-54a4d3a421e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 1991 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_MNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b713807-995c-4039-80be-43a58d7e7cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(7, -1, 1, 40_000, 25, 1_200, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b2e5bda-0cc4-4dda-90cc-1d2d7eb0f85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_MNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a2e2874-dc87-4a1a-a26f-baa4f3a6c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=7, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b082049-8f74-4bae-9669-385b4814c820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_MNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e51bfc23-ff74-4d16-bb27-6ccf973ea4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76214"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS5_2_2_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59069f-461d-43f6-a53a-9a04d7e04183",
   "metadata": {},
   "source": [
    "### Black box 3: Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5cabde6-88b9-49da-b676-fb5c831a19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation = \"relu\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(32, activation = \"relu\"),\n",
    "        layers.Dense(16, activation = \"relu\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "bbmodel.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy())\n",
    "bbmodel.fit(X_train, y_train, batch_size=32, epochs=50, verbose = 0)\n",
    "\n",
    "yhat = bbmodelW(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == (2*y_test-1))\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 7))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c59c3be6-19da-45ad-89c3-daf97d4a091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 3375 points in 240.04 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(7, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d539c913-ee15-4402-8c84-a58f289504d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 3376 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_MNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92443e74-8c5c-495c-a34f-9768cf2999f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(7, -1, 1, 40_000, 25, 1_200, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "312433bf-d290-4d46-a242-059e6ac7662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_MNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b869f7c-fdc5-4259-98df-cb3d9c051e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=7, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4e2cd6f-40b7-40fc-beef-b03c1be3066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_MNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1d37bb6-9675-4bb6-bcc8-5efd45bf2cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78986"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS5_3_2_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a699d43-52e6-4bc9-aa88-b45b08aa93d0",
   "metadata": {},
   "source": [
    "## Connectionist bench (mines vs rocks) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cf8db7c-00a1-4c47-a298-40cefe70402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 166\n",
      "Size of test dataset: 42\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "connectionist_bench_sonar_mines_vs_rocks = fetch_ucirepo(id=151) \n",
    "\n",
    "X = connectionist_bench_sonar_mines_vs_rocks.data.features.values \n",
    "X = normalize(X) \n",
    "y = np.where(connectionist_bench_sonar_mines_vs_rocks.data.targets.values.flatten() == 'M', 1.,0.)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "print(\"Size of training dataset:\", len(X_train))\n",
    "print(\"Size of test dataset:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e08e21-346a-47b0-9c5b-a037b0dc16c0",
   "metadata": {},
   "source": [
    "### Black box 1: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7c40b85-029d-4af8-8b1f-05ba9fdea0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = RandomForestClassifier(max_depth=10, min_samples_leaf=5)\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 60))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4de92dc3-8bdd-4a77-886a-152a01862c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 1417 points in 240.09 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(60, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d308829-a8a8-42d0-91ef-3a3163e9dcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 1418 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_MNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51a50cbe-c48c-4cb5-bc98-850a4a5e426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(60, -1, 1, 40_000, 25, 1_200, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c4793ee-5782-447c-b24c-f519a84bfcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_MNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "907a5b1b-8df7-4235-86ac-69d17032fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=60, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6ebe940-8161-4e52-a32a-5e05cba53b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_MNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da91f446-3332-4b82-bf14-5167c13c715f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80591"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS6_1_2_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dc1c6-880b-4198-9d6c-2e1cd3aa76b1",
   "metadata": {},
   "source": [
    "### Black box 2: Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6909dcbd-a30a-46b4-aa39-cfdd8c18da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = HistGradientBoostingClassifier()\n",
    "bbmodel.fit(X_train, y_train)\n",
    "yhat = bbmodel.predict(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == y_test)\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 60))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a89a1730-8dc9-4289-90c9-7ac186c3ba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 1493 points in 240.12 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(60, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "19b25ad0-833f-413d-8bd9-49fa8f2820f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 1494 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_MNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8f92488-4d88-4d8e-b83f-3e2f67c07af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 800025 points in 240.03 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(60, -1, 1, 40_000, 25, 1_200, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4dfc465-2ce9-4b58-8a84-f8d99a2eca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 800025 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_MNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19360180-1280-4605-84e8-14248a38e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=60, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2a3d20f-6efb-4f1f-9eba-660a8588b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_MNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a9a59b0-f466-459d-a9e6-83bea6ac2798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75161"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS6_2_2_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ea2c6-8c54-4606-815e-0ad1337013bf",
   "metadata": {},
   "source": [
    "### Black box 3: Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e1e7165-742d-43d0-8f56-dbdd73169d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, train and evaluate the black box\n",
    "bbmodel = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(128, activation = \"relu\"),\n",
    "        layers.Dense(64, activation = \"relu\"),\n",
    "        layers.Dense(32, activation = \"relu\"),\n",
    "        layers.Dense(16, activation = \"relu\"),\n",
    "        layers.Dense(1, activation = \"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "bbmodel.compile(optimizer=\"adam\", loss=keras.losses.BinaryCrossentropy())\n",
    "bbmodel.fit(X_train, y_train, batch_size=32, epochs=50, verbose=0)\n",
    "\n",
    "yhat = bbmodelW(X_test)\n",
    "\n",
    "# Accuracy of the black box\n",
    "accbb = np.mean(yhat == (2*y_test-1))\n",
    "\n",
    "# We generate 1,000,000 points uniformly to test the copy. \n",
    "data_test_syn = np.random.uniform(-1,1, (1000000, 60))\n",
    "\n",
    "# We label these points with labels 1 and -1\n",
    "y_test_syn = bbmodelW(data_test_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1912bdda-55b2-435d-bc91-bb2836a63ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 50 points\n",
      "We have labelled 200 points\n",
      "We have labelled 500 points\n",
      "We have labelled 1000 points\n",
      "We have labelled 2338 points in 240.04 seconds\n"
     ]
    }
   ],
   "source": [
    "pts_1, data_1, lab_1 = generate_distances_algo1(60, -1, 1, 3, 2, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe04a224-265c-40af-85c5-1b65b3a31f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 2339 points done\n"
     ]
    }
   ],
   "source": [
    "efe_1, acc_1, efe_unif_1, model1 = train_copy_MNNd(pts_1, data_1, lab_1, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "530fda38-e1e3-477a-83ca-5a9c8a1cf6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have labelled 10000 points\n",
      "We have labelled 50000 points\n",
      "We have labelled 200000 points\n",
      "We have labelled 400000 points\n",
      "We have labelled 600000 points\n",
      "We have labelled 800000 points\n",
      "We have labelled 1000000 points\n"
     ]
    }
   ],
   "source": [
    "pts_2, data_2, lab_2 = generate_distances_algo2(60, -1, 1, 40_000, 25, 1_200, 2, 1, bbmodelW, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be462cb5-c949-40b7-8df7-8542c53fe0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_2, acc_2, efe_unif_2, model2 = train_copy_MNNd(pts_2, data_2, lab_2, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "109f088f-083e-436b-8eb9-bfe969df4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pow2 = next_power_of_2(1_000_000)\n",
    "sampler = qmc.Sobol(d=60, scramble=False)\n",
    "sobol_points = sampler.random_base2(m=int(np.log2(n_pow2)))\n",
    "sobol_points = sobol_points[:1_000_000]\n",
    "data_3 = 2*sobol_points - 1\n",
    "lab_3 = bbmodelW(data_3)\n",
    "\n",
    "pts_3 = [50, 200, 500, 1_000, 5_000, 10_000, 50_000, 200_000, 400_000, 600_000, 800_000, 1_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32ba977b-922c-443a-8c20-b6a96836649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations for 50 points done\n",
      "Computations for 200 points done\n",
      "Computations for 500 points done\n",
      "Computations for 1000 points done\n",
      "Computations for 5000 points done\n",
      "Computations for 10000 points done\n",
      "Computations for 50000 points done\n",
      "Computations for 200000 points done\n",
      "Computations for 400000 points done\n",
      "Computations for 600000 points done\n",
      "Computations for 800000 points done\n",
      "Computations for 1000000 points done\n"
     ]
    }
   ],
   "source": [
    "efe_3, acc_3, efe_unif_3, model3 = train_copy_MNNh(data_3, lab_3, X_test, y_test, data_test_syn, y_test_syn, bbmodelW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b9822ad-130b-466a-8c9c-7d0d01e95f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79142"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_store = {}\n",
    "\n",
    "for i in range(1, 4):\n",
    "    model_key = f\"model{i}\"\n",
    "    data_to_store[model_key] = {\n",
    "        \"model\": globals()[f\"model{i}\"],\n",
    "        \"pts\": globals()[f\"pts_{i}\"],\n",
    "        \"efe\": globals()[f\"efe_{i}\"],\n",
    "        \"acc\": globals()[f\"acc_{i}\"],\n",
    "        \"efe_unif\": globals()[f\"efe_unif_{i}\"]\n",
    "    }\n",
    "\n",
    "data_to_store[\"blackb\"] = {\n",
    "    \"model\": bbmodel,\n",
    "    \"acc\": accbb\n",
    "}\n",
    "\n",
    "filename = f\"../results/results_DS6_3_2_seed{seed}.pkl\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(data_to_store, f)\n",
    "\n",
    "data_to_store.clear()\n",
    "del data_to_store\n",
    "for i in range(1, 4):\n",
    "    del globals()[f\"model{i}\"]\n",
    "    del globals()[f\"pts_{i}\"]\n",
    "    del globals()[f\"efe_{i}\"]\n",
    "    del globals()[f\"acc_{i}\"]\n",
    "    del globals()[f\"efe_unif_{i}\"]\n",
    "del bbmodel\n",
    "del accbb\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
